{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmW8qDKm+lbRKKnYBc8sJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishekconfused/Models/blob/main/Copy_of_logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeszsNbyFbOS",
        "outputId": "4430419f-ab64-4ae6-f733-6b9539fe5d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#my project starts here\n",
        "import numpy as np #array\n",
        "import pandas as pd #table\n",
        "import matplotlib.pyplot as plt #plots\n",
        "import re #substitution\n",
        "import random #for pusedo random numbers\n",
        "import nltk #natural language toolkit\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfgZqVyrFpv5",
        "outputId": "ee33bc60-c607-41c8-d8d4-519f0829cea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "S5UAxZAKdhVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for preprocessing. a tweet contains various useless things which are not necessary for our model.I will remove them one by one.\n",
        "def preprocess(tweet):\n",
        "  # remove old style retweet text \"RT\"\n",
        "  tweett = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # remove hyperlinks\n",
        "  tweettt = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweett)\n",
        "\n",
        "  # remove hashtags\n",
        "  # only removing the hash # sign from the word\n",
        "  tweetttt = re.sub(r'#', '', tweettt)\n",
        "  tweet2= re.sub(r',','',tweetttt)\n",
        "\n",
        "  #lets tokenize that is break the tweet\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                                reduce_len=True)\n",
        "  tweet3= tokenizer.tokenize(tweet2)\n",
        "    #remove stopwords and panctuations\n",
        "  import string\n",
        "  stopwords=nltk.corpus.stopwords.words('english')\n",
        "  tweet4=[]\n",
        "  for words in tweet3:\n",
        "    if words not in stopwords and words not in string.punctuation:\n",
        "        tweet4.append(words)\n",
        "  #lets stem\n",
        "  stemmer=PorterStemmer()\n",
        "  tweet5=[]\n",
        "  for words in tweet4:\n",
        "    stem=stemmer.stem(words)\n",
        "    tweet5.append(stem)\n",
        "   #lets lematize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweet6=[]\n",
        "\n",
        "  for word in tweet5:\n",
        "      lem_word = lemmatizer.lemmatize(word)  # stemming word\n",
        "      tweet6.append(lem_word)  # append to the list\n",
        "\n",
        "  return tweet6"
      ],
      "metadata": {
        "id": "Lu6skD3J2n21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequency generating function\n",
        "#here I will use it to define frequency of a word in posoitive tweert and negative tweet. It will help me to analyse and train the model.\n",
        "def countfreqs(tweet, ys):\n",
        "    yslist = np.squeeze(ys).tolist()#the ys which is taken as input is squeezed so that it doesnot be a single element.\n",
        "\n",
        "    freqs = {}#here I wil pair words with their frequency in a particular emotion\n",
        "    for y, tweet in zip(yslist, tweet):\n",
        "        for word in preprocess(tweet):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "\n",
        "    return freqs\n"
      ],
      "metadata": {
        "id": "kYHJjFZ3F2nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#logical regression model requires sigmoid function so that I can get things between -1 and +1.basically as a number less than 1.\n",
        "def sigmoid(z):\n",
        "  h=1/(1+np.exp(-z))\n",
        "  return h"
      ],
      "metadata": {
        "id": "PJhPnTTjF2v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "\n",
        "    m = len(x)\n",
        "\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "\n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m)*np.dot(x.T, h-y)\n",
        "\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "AXHGal4rF23M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "  words= preprocess(tweet)\n",
        "  x = np.zeros((1, 3))#creating an array of size (1,3)\n",
        "  x[0,0] = 1#I am setting the first term called bias term as 1.\n",
        "  for word in words:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word,1),0)\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word,0),0)\n",
        "\n",
        "  assert(x.shape == (1, 3))\n",
        "  return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5fjN8TPmHsSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING."
      ],
      "metadata": {
        "id": "CvTWjg3VIKBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p=twitter_samples.strings('positive_tweets.json')\n",
        "n=twitter_samples.strings('negative_tweets.json')\n",
        "t=p+n\n",
        "y = np.append(np.ones((len(p), 1)), np.zeros((len(n), 1)), axis=0)"
      ],
      "metadata": {
        "id": "0mMnLpIJIP3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I will train the data using 90:10 ratio.\n",
        "test_pos = p[4500:]\n",
        "train_pos = p[:4500]\n",
        "test_neg = n[4500:]\n",
        "train_neg = n[:4500]\n",
        "train_x = train_neg+train_pos\n",
        "test_x = test_pos + test_neg\n",
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "spt5v8pMIaSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs=countfreqs(train_x,train_y)#I am gettint the freqs dictionary using the function ."
      ],
      "metadata": {
        "id": "Tlj_vAtLI-bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "Y = Y.reshape((len(train_y), 1))\n",
        "# # Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n"
      ],
      "metadata": {
        "id": "VMcerS-mIgzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tweet, freqs, theta):# a fuction which will work on tweets and give me output in a number less than 1.\n",
        "  x = extract_features(tweet, freqs)#I am storing the features extracted from a tweet in x\n",
        "\n",
        "  # make the prediction using x and theta\n",
        "  z = np.dot(x,theta)#I am doing dot of theta fuction defined for the trained data and combining it with the features extracted from the x\n",
        "  predict = sigmoid(z)#sigmoid function defined above\n",
        "  return predict\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1DEzqriJICd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "  y_hat = []# storing predictions\n",
        "\n",
        "  for tweet in test_x:\n",
        "        y_pred = predict(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1) # append 1.0 to the list\n",
        "        else:\n",
        "            y_hat.append(0)# append 0 to the list\n",
        "  y_hat = np.array(y_hat)#converting list in array\n",
        "  test_y = test_y.reshape(-1)\n",
        "  accuracy = np.sum((test_y == y_hat).astype(int))/len(test_x)# I will get my predictions in the form of decimal.\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qS3sjeS0JQMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7fWJUUKhjsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
