{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSWp0H8c40S+CnkS4l6DIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishekconfused/Models/blob/main/bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my project starts here\n",
        "import numpy as np #array\n",
        "import pandas as pd #table\n",
        "import matplotlib.pyplot as plt #plots\n",
        "import re #substitution\n",
        "import random #for pusedo random numbers\n",
        "import nltk #natural language toolkit\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import collections\n",
        "!pip install utils\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncO3j9zgFYJ1",
        "outputId": "b4a2a9b0-073e-4ea5-d555-9668e6c49ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lookup"
      ],
      "metadata": {
        "id": "vp5e9Z57FInq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17adbe48-a860-4d00-8f61-9aa4c6e2a79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lookup in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from lookup) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lookup) (2.31.0)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->lookup) (4.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->lookup) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lookup) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lookup) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lookup) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lookup) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lookup import lookup"
      ],
      "metadata": {
        "id": "wGFW3icoTLCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d95vDfAYDRSo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pdb\n",
        "from os import getcwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function for preprocessing. a tweet contains various useless things which are not necessary for our model.I will remove them one by one.\n",
        "def preprocess(tweet):\n",
        "  # remove old style retweet text \"RT\"\n",
        "  tweett = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # remove hyperlinks\n",
        "  tweettt = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweett)\n",
        "\n",
        "  # remove hashtags\n",
        "  # only removing the hash # sign from the word\n",
        "  tweetttt = re.sub(r'#', '', tweettt)\n",
        "  tweet2= re.sub(r',','',tweetttt)\n",
        "\n",
        "  #lets tokenize that is break the tweet\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                                reduce_len=True)\n",
        "  tweet3= tokenizer.tokenize(tweet2)\n",
        "    #remove stopwords and panctuations\n",
        "  import string\n",
        "  stopwords=nltk.corpus.stopwords.words('english')\n",
        "  tweet4=[]\n",
        "  for words in tweet3:\n",
        "    if words not in stopwords and words not in string.punctuation:\n",
        "        tweet4.append(words)\n",
        "  #lets stem\n",
        "  stemmer=PorterStemmer()\n",
        "  tweet5=[]\n",
        "  for words in tweet4:\n",
        "    stem=stemmer.stem(words)\n",
        "    tweet5.append(stem)\n",
        "   #lets lematize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tweet6=[]\n",
        "\n",
        "  for word in tweet5:\n",
        "      lem_word = lemmatizer.lemmatize(word)  # stemming word\n",
        "      tweet6.append(lem_word)  # append to the list\n",
        "\n",
        "\n",
        "  return tweet6\n"
      ],
      "metadata": {
        "id": "jgiGrX7-ExmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def count_tweets(result,tweets,ys):\n",
        "#   preprocess(tweets)\n",
        "#   for y, tweet in zip(ys,tweets):\n",
        "#     for word in preprocess(tweet):\n",
        "#       pair=(word,y)\n",
        "#       if pair in result:\n",
        "#         result[pair]+=1\n",
        "#       else:\n",
        "#         result[pair]=1\n",
        "#   return result\n",
        "# def count_tweets(result, tweets, ys):\n",
        "#     if not isinstance(tweets, str):\n",
        "#         tweets = str(tweets)\n",
        "#     preprocess(tweets)\n",
        "#     for y, tweet in zip(ys, tweets):\n",
        "#         for word in preprocess(tweet):\n",
        "#             pair = (word, y)\n",
        "#             if pair not in result:\n",
        "#                 result[pair] = 0\n",
        "#             result[pair] += 1\n",
        "def build_freqs(tweet, ys):\n",
        "    yslist = np.squeeze(ys).tolist()#the ys which is taken as input is squeezed so that it doesnot be a single element.\n",
        "\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweet):\n",
        "        for word in preprocess(tweet):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "\n",
        "    return freqs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jqgvbD_LIeLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=twitter_samples.strings('positive_tweets.json')\n",
        "n=twitter_samples.strings('negative_tweets.json')\n",
        "t=p+n\n",
        "y = np.append(np.ones((len(p), 1)), np.zeros((len(n), 1)), axis=0)"
      ],
      "metadata": {
        "id": "yWxzPWw_J482"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I will train the data using 90:10 ratio.\n",
        "test_pos = p[4500:]\n",
        "train_pos = p[:4500]\n",
        "test_neg = n[4500:]\n",
        "train_neg = n[:4500]\n",
        "train_x = train_neg+train_pos\n",
        "test_x = test_pos + test_neg\n",
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "s10CCGW6J0ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs=build_freqs(train_x,train_y)\n"
      ],
      "metadata": {
        "id": "PDuqtimyJ9OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "    # calculate N_pos and N_neg\n",
        "    N_pos = N_neg = 0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs.get(pair, 1)\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs.get(pair, 1)\n",
        "    # Calculate D, the number of documents\n",
        "    D = len(train_y)\n",
        "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
        "    D_pos = sum(train_y)\n",
        "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
        "    D_neg = D-D_pos\n",
        "    # Calculate logprior\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = freqs.get((word, 1),0)\n",
        "        freq_neg = freqs.get((word, 0),0)\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos + 1)/(N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1)/(N_neg + V)\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "    return logprior, loglikelihood\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n"
      ],
      "metadata": {
        "id": "g1k3A47BE7f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "\n",
        "  # process the tweet to get a list of words\n",
        "  word_l = preprocess(tweet)\n",
        "  # initialize probability to zero\n",
        "  p = 0\n",
        "  # add the logprior\n",
        "  p += logprior\n",
        "  for word in word_l:\n",
        "# check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood[word]\n",
        "  return p"
      ],
      "metadata": {
        "id": "2zX0iIpmSsDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "\n",
        "    accuracy = 0  # return this properly\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "# append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    error = np.mean(np.absolute(y_hats - test_y))\n",
        "# Accuracy is 1 minus the error\n",
        "    accuracy = 1 - error\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "eRH1GhslkXWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTOMU-Jgkeyp",
        "outputId": "537a8502-f476-4361-e099-0beb4635ba87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}